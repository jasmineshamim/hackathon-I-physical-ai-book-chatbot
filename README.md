# Physical AI RAG Chatbot

A RAG-based AI chatbot that uses Qdrant vector storage and Google's Gemini AI to answer questions about physical AI concepts.

## ğŸ—ï¸ Project Structure

```
project-root/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ chat.py
â”‚   â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”‚   â”œâ”€â”€ ingest.py
â”‚   â”‚   â”‚   â”œâ”€â”€ query.py
â”‚   â”‚   â”‚   â””â”€â”€ vector_store.py
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/ (Next.js App Router)
â”‚   â”‚   â”œâ”€â”€ page.tsx
â”‚   â”‚   â”œâ”€â”€ layout.tsx
â”‚   â”‚   â””â”€â”€ chat/
â”‚   â”‚       â””â”€â”€ page.tsx
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ChatWindow.tsx
â”‚   â”‚   â”œâ”€â”€ MessageBubble.tsx
â”‚   â”‚   â””â”€â”€ InputBox.tsx
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â””â”€â”€ api.ts
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tailwind.config.js
â”‚
â””â”€â”€ README.md
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- Node.js 18+
- Qdrant vector database
- Google Gemini API key

### Backend Setup

1. Navigate to the backend directory:
   ```bash
   cd backend
   ```

2. Create a virtual environment:
   ```bash
   python -m venv venv
   ```

3. Activate the virtual environment:
   ```bash
   # On Windows
   venv\Scripts\activate
   
   # On macOS/Linux
   source venv/bin/activate
   ```

4. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

5. Set up your environment variables in `.env`:
   ```env
   GEMINI_API_KEY=your-gemini-api-key-here
   QDRANT_API_KEY=your-qdrant-api-key-here
   QDRANT_URL=http://localhost:6333
   QDRANT_COLLECTION_NAME=documents
   GEMINI_MODEL_NAME=gemini-pro
   RETRIEVAL_LIMIT=5
   ```

6. Run the backend server:
   ```bash
   uvicorn app.main:app --reload
   ```

### Frontend Setup

1. Navigate to the frontend directory:
   ```bash
   cd frontend
   ```

2. Install dependencies:
   ```bash
   npm install
   ```

3. Create a `.env.local` file in the frontend directory:
   ```env
   NEXT_PUBLIC_API_BASE_URL=http://localhost:8000/api/v1
   ```

4. Run the development server:
   ```bash
   npm run dev
   ```

5. Open your browser to [http://localhost:3000](http://localhost:3000) to view the app.

## ğŸ› ï¸ API Endpoints

- `GET /health` - Check if the API is running
- `POST /api/v1/chat` - Send a message and get a response from the AI

## ğŸ¤– Features

- RAG (Retrieval Augmented Generation) powered by Qdrant and Gemini
- Chat interface with message history
- Loading indicators
- Source attribution for AI responses

## ğŸ“ What Was Fixed

1. **Separated backend and frontend completely** - No more mixing of files
2. **Backend** - FastAPI with RAG functionality using Qdrant and Gemini
3. **Frontend** - Next.js with clean, responsive chat UI
4. **Proper folder structure** - Following industry standards
5. **Environment configuration** - Proper .env handling

## ğŸ§ª Testing

To test the backend API, you can use curl:

```bash
curl -X POST "http://localhost:8000/api/v1/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What is physical AI?",
    "history": []
  }'
```

## ğŸš§ Troubleshooting

- Make sure Qdrant is running at the specified URL
- Verify your Gemini API key is valid and has the necessary permissions
- Check that both backend and frontend are running on their respective ports